{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e0ae086-e30d-4890-bbcc-6c3ed64ec6dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T23:52:32.917695Z",
     "iopub.status.busy": "2026-02-17T23:52:32.917046Z",
     "iopub.status.idle": "2026-02-17T23:52:40.063662Z",
     "shell.execute_reply": "2026-02-17T23:52:40.062695Z",
     "shell.execute_reply.started": "2026-02-17T23:52:32.917667Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "# Use only 1 GPU if available\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from chronos import BaseChronosPipeline, Chronos2Pipeline\n",
    "\n",
    "# Load the Chronos-2 pipeline\n",
    "# GPU recommended for faster inference, but CPU is also supported using device_map=\"cpu\"\n",
    "pipeline: Chronos2Pipeline = BaseChronosPipeline.from_pretrained(\"amazon/chronos-2\", device_map=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c66f69a1-6b7c-4d3a-9b27-a3f43ecdf56c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T23:52:40.064503Z",
     "iopub.status.busy": "2026-02-17T23:52:40.064180Z",
     "iopub.status.idle": "2026-02-17T23:54:27.478156Z",
     "shell.execute_reply": "2026-02-17T23:54:27.477339Z",
     "shell.execute_reply.started": "2026-02-17T23:52:40.064485Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58327365</th>\n",
       "      <td>FOODS_3_823_WI_3</td>\n",
       "      <td>2016-04-24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58327366</th>\n",
       "      <td>FOODS_3_824_WI_3</td>\n",
       "      <td>2016-04-24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58327367</th>\n",
       "      <td>FOODS_3_825_WI_3</td>\n",
       "      <td>2016-04-24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58327368</th>\n",
       "      <td>FOODS_3_826_WI_3</td>\n",
       "      <td>2016-04-24</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58327369</th>\n",
       "      <td>FOODS_3_827_WI_3</td>\n",
       "      <td>2016-04-24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58327370 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id  timestamp  value\n",
       "0         HOBBIES_1_001_CA_1 2011-01-29      0\n",
       "1         HOBBIES_1_002_CA_1 2011-01-29      0\n",
       "2         HOBBIES_1_003_CA_1 2011-01-29      0\n",
       "3         HOBBIES_1_004_CA_1 2011-01-29      0\n",
       "4         HOBBIES_1_005_CA_1 2011-01-29      0\n",
       "...                      ...        ...    ...\n",
       "58327365    FOODS_3_823_WI_3 2016-04-24      1\n",
       "58327366    FOODS_3_824_WI_3 2016-04-24      0\n",
       "58327367    FOODS_3_825_WI_3 2016-04-24      0\n",
       "58327368    FOODS_3_826_WI_3 2016-04-24      3\n",
       "58327369    FOODS_3_827_WI_3 2016-04-24      0\n",
       "\n",
       "[58327370 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"m5-forecasting-accuracy/sales_train_validation.csv\")\n",
    "df[\"id\"] = df[\"item_id\"] + \"_\" + df[\"store_id\"]\n",
    "df = df[[\"id\", ] + [cn for cn in df.columns if cn.startswith(\"d_\")]]\n",
    "\n",
    "df = df.melt(id_vars=\"id\", var_name=\"timestamp\", value_name=\"value\")\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"].apply(\n",
    "    lambda x: datetime.datetime(2011, 1, 28) + datetime.timedelta(days=int(str(x).replace(\"d_\", \"\")))))\n",
    "\n",
    "# df[\"value\"] = np.log1p(df[\"value\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da78ef72-4e80-4f4c-b303-3d5a090f9d95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T23:54:27.478759Z",
     "iopub.status.busy": "2026-02-17T23:54:27.478621Z",
     "iopub.status.idle": "2026-02-17T23:56:27.363495Z",
     "shell.execute_reply": "2026-02-17T23:56:27.362692Z",
     "shell.execute_reply.started": "2026-02-17T23:54:27.478744Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59181085</th>\n",
       "      <td>FOODS_3_823_WI_3</td>\n",
       "      <td>2016-05-22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59181086</th>\n",
       "      <td>FOODS_3_824_WI_3</td>\n",
       "      <td>2016-05-22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59181087</th>\n",
       "      <td>FOODS_3_825_WI_3</td>\n",
       "      <td>2016-05-22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59181088</th>\n",
       "      <td>FOODS_3_826_WI_3</td>\n",
       "      <td>2016-05-22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59181089</th>\n",
       "      <td>FOODS_3_827_WI_3</td>\n",
       "      <td>2016-05-22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59181090 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id  timestamp  value\n",
       "0         HOBBIES_1_001_CA_1 2011-01-29      0\n",
       "1         HOBBIES_1_002_CA_1 2011-01-29      0\n",
       "2         HOBBIES_1_003_CA_1 2011-01-29      0\n",
       "3         HOBBIES_1_004_CA_1 2011-01-29      0\n",
       "4         HOBBIES_1_005_CA_1 2011-01-29      0\n",
       "...                      ...        ...    ...\n",
       "59181085    FOODS_3_823_WI_3 2016-05-22      1\n",
       "59181086    FOODS_3_824_WI_3 2016-05-22      0\n",
       "59181087    FOODS_3_825_WI_3 2016-05-22      2\n",
       "59181088    FOODS_3_826_WI_3 2016-05-22      0\n",
       "59181089    FOODS_3_827_WI_3 2016-05-22      1\n",
       "\n",
       "[59181090 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"m5-forecasting-accuracy/sales_train_evaluation.csv\")\n",
    "test_df[\"id\"] = test_df[\"item_id\"] + \"_\" + test_df[\"store_id\"]\n",
    "test_df = test_df[[\"id\", ] + [cn for cn in test_df.columns if cn.startswith(\"d_\")]]\n",
    "test_df = test_df.melt(id_vars=\"id\", var_name=\"timestamp\", value_name=\"value\")\n",
    "\n",
    "test_df[\"timestamp\"] = pd.to_datetime(test_df[\"timestamp\"].apply(\n",
    "    lambda x: datetime.datetime(2011, 1, 28) + datetime.timedelta(days=int(str(x).replace(\"d_\", \"\")))))\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7a44539-d2af-4af4-a028-527643776f5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T23:56:27.364264Z",
     "iopub.status.busy": "2026-02-17T23:56:27.364130Z",
     "iopub.status.idle": "2026-02-17T23:56:49.277794Z",
     "shell.execute_reply": "2026-02-17T23:56:49.277082Z",
     "shell.execute_reply.started": "2026-02-17T23:56:27.364251Z"
    }
   },
   "outputs": [],
   "source": [
    "train_inputs = []\n",
    "for item_id, group in df.groupby(\"id\"):\n",
    "    train_inputs.append({\n",
    "        \"target\": group[\"value\"].values,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1aa87a5-b464-426a-b84d-575361832955",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-17T23:57:16.065362Z",
     "iopub.status.busy": "2026-02-17T23:57:16.065146Z",
     "iopub.status.idle": "2026-02-17T23:57:18.328084Z",
     "shell.execute_reply": "2026-02-17T23:57:18.327213Z",
     "shell.execute_reply.started": "2026-02-17T23:57:16.065345Z"
    }
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.72 GiB of which 19.44 MiB is free. Process 317060 has 15.70 GiB memory in use. Of the allocated memory 15.29 GiB is allocated by PyTorch, and 212.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Fine-tune the model by default full fine-tuning will be performed\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m finetuned_pipeline = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m28\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.12/site-packages/chronos/chronos2/pipeline.py:197\u001b[39m, in \u001b[36mChronos2Pipeline.fit\u001b[39m\u001b[34m(self, inputs, prediction_length, validation_inputs, finetune_mode, lora_config, context_length, learning_rate, num_steps, batch_size, output_dir, min_past, finetuned_ckpt_name, callbacks, remove_printer_callback, disable_data_parallel, **extra_trainer_kwargs)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;66;03m# Create a copy of the model to avoid modifying the original\u001b[39;00m\n\u001b[32m    196\u001b[39m config = deepcopy(\u001b[38;5;28mself\u001b[39m.model.config)\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m model = \u001b[43mChronos2Model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    198\u001b[39m model.load_state_dict(\u001b[38;5;28mself\u001b[39m.model.state_dict())\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m finetune_mode == \u001b[33m\"\u001b[39m\u001b[33mlora\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4343\u001b[39m, in \u001b[36mPreTrainedModel.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   4338\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[32m   4339\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   4340\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4341\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m `dtype` by passing the correct `dtype` argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4342\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m4343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1371\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1368\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1369\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: Module._apply at line 930 (4 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    935\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    941\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:957\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    953\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    954\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    955\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    958\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1357\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1350\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1351\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1352\u001b[39m             device,\n\u001b[32m   1353\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1354\u001b[39m             non_blocking,\n\u001b[32m   1355\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1356\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1363\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.72 GiB of which 19.44 MiB is free. Process 317060 has 15.70 GiB memory in use. Of the allocated memory 15.29 GiB is allocated by PyTorch, and 212.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model by default full fine-tuning will be performed\n",
    "finetuned_pipeline = pipeline.fit(\n",
    "    inputs=train_inputs,\n",
    "    prediction_length=28,\n",
    "    num_steps=1000, \n",
    "    learning_rate=1e-6,\n",
    "    batch_size=32,\n",
    "    logging_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cd926e-9872-49f5-9dc6-3cde6c39753e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-17T23:56:56.841092Z",
     "iopub.status.idle": "2026-02-17T23:56:56.841776Z",
     "shell.execute_reply": "2026-02-17T23:56:56.841196Z",
     "shell.execute_reply.started": "2026-02-17T23:56:56.841188Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "# Visualization helper function\n",
    "def plot_forecast(\n",
    "    context_df: pd.DataFrame,\n",
    "    pred_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    target_column: str,\n",
    "    timeseries_id: str,\n",
    "    id_column: str = \"id\",\n",
    "    timestamp_column: str = \"timestamp\",\n",
    "    history_length: int = 256,\n",
    "    title_suffix: str = \"\",\n",
    "):\n",
    "    ts_context = context_df.query(f\"{id_column} == @timeseries_id\").set_index(timestamp_column)[target_column]\n",
    "    ts_pred = pred_df.query(f\"{id_column} == @timeseries_id and target_name == @target_column\").set_index(\n",
    "        timestamp_column\n",
    "    )[[\"0.1\", \"predictions\", \"0.9\"]]\n",
    "    ts_ground_truth = test_df.query(f\"{id_column} == @timeseries_id\").set_index(timestamp_column)[target_column]\n",
    "\n",
    "    last_date = ts_context.index.max()\n",
    "    start_idx = max(0, len(ts_context) - history_length)\n",
    "    plot_cutoff = ts_context.index[start_idx]\n",
    "    ts_context = ts_context[ts_context.index >= plot_cutoff]\n",
    "    ts_pred = ts_pred[ts_pred.index >= plot_cutoff]\n",
    "    ts_ground_truth = ts_ground_truth[ts_ground_truth.index >= plot_cutoff]\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 3))\n",
    "    ax = fig.gca()\n",
    "    ts_context.plot(ax=ax, label=f\"historical {target_column}\", color=\"xkcd:azure\")\n",
    "    ts_ground_truth.plot(ax=ax, label=f\"future {target_column} (ground truth)\", color=\"xkcd:grass green\")\n",
    "    ts_pred[\"predictions\"].plot(ax=ax, label=\"forecast\", color=\"xkcd:violet\")\n",
    "    ax.fill_between(\n",
    "        ts_pred.index,\n",
    "        ts_pred[\"0.1\"],\n",
    "        ts_pred[\"0.9\"],\n",
    "        alpha=0.7,\n",
    "        label=\"prediction interval\",\n",
    "        color=\"xkcd:light lavender\",\n",
    "    )\n",
    "    ax.axvline(x=last_date, color=\"black\", linestyle=\"--\", alpha=0.5)\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    ax.set_title(f\"{target_column} forecast for {timeseries_id} {title_suffix}\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02da489b-8e46-4844-8d92-ab41c46b3b20",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-17T23:56:56.843208Z",
     "iopub.status.idle": "2026-02-17T23:56:56.844350Z",
     "shell.execute_reply": "2026-02-17T23:56:56.843402Z",
     "shell.execute_reply.started": "2026-02-17T23:56:56.843385Z"
    }
   },
   "outputs": [],
   "source": [
    "id = df.groupby(\"id\")[\"value\"].mean().idxmax()\n",
    "\n",
    "# Use the fine-tuned model for predictions\n",
    "finetuned_pred_df = finetuned_pipeline.predict_df(\n",
    "    df,\n",
    "    prediction_length=28,\n",
    "    quantile_levels=[0.1, 0.5, 0.9],\n",
    "    id_column=\"id\",\n",
    "    timestamp_column=\"timestamp\",\n",
    "    target=\"value\",\n",
    ")\n",
    "\n",
    "plot_forecast(\n",
    "    df,\n",
    "    finetuned_pred_df,\n",
    "    test_df,\n",
    "    target_column=\"value\",\n",
    "    timeseries_id=id,\n",
    "    title_suffix=\"(full fine-tuned)\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
